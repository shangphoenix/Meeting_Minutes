# app.py
# =========================================================
# WebSocket å®æ—¶è½¬å†™ + æ–­å¼€åç¦»çº¿è¯´è¯äººåˆå¹¶+ DeepSeek æ€»ç»“
# =========================================================
# åŠŸèƒ½æ¦‚è¿°ï¼š
#  - æ¥æ”¶å‰ç«¯é€šè¿‡ WebSocket å‘é€çš„äºŒè¿›åˆ¶ PCM16LE éŸ³é¢‘æµï¼ˆ16kHz å•å£°é“ï¼‰ï¼Œ
#    å®æ—¶åš VAD -> ASR å¹¶å°†ä¸­é—´åˆ†æ®µè¯†åˆ«ç»“æœå›ä¼ ç½‘é¡µã€‚
#  - åœ¨æ”¶åˆ°å®¢æˆ·ç«¯ã€Œendã€ä¿¡å·æˆ–ç»“æŸåï¼Œä¿å­˜å®Œæ•´ WAVï¼Œ
#    ç¦»çº¿æ‰§è¡Œ ASR/VAD/PUNC/è¯´è¯äººèšç±»ï¼Œç”Ÿæˆæœ€ç»ˆ JSON å¹¶å›ä¼ ã€‚
#  - ç¦»çº¿å®Œæˆåè°ƒç”¨ DeepSeekï¼ˆæœ¬åœ° Ollama æˆ–äº‘ç«¯ APIï¼‰ç”Ÿæˆä¼šè®®çºªè¦ summary å¹¶å›ä¼ ã€‚
#
# è¾“å…¥ï¼ˆWebSocketï¼‰ï¼š
#  - äºŒè¿›åˆ¶ï¼šPCM16LE, 16kHz, mono
#  - æ–‡æœ¬ï¼š{"type":"end"} æˆ– çº¯å­—ç¬¦ä¸² "end" è¡¨ç¤ºå½•éŸ³ç»“æŸ
#
# è¾“å‡ºï¼ˆWebSocket JSON messageï¼‰ï¼š
#  - code=0: å®æ—¶åˆ†æ®µè¯†åˆ«ç»“æœï¼ˆdata=textï¼Œinfo åŒ…å«æ—¶é—´æˆ³/asrè€—æ—¶ç­‰ï¼‰
#  - code=1: æ–­å¼€åç¦»çº¿è¯´è¯äººåˆå¹¶è¾“å‡ºï¼ˆdata=æœ€ç»ˆ JSONï¼‰
#  - code=2: DeepSeek ç”Ÿæˆçš„ summary æ–‡æœ¬
#
# æœ¬åœ°è½ç›˜ï¼ˆæ¯æ¬¡è¿æ¥ç”Ÿæˆç‹¬ç«‹ session ç›®å½•ï¼‰ï¼š
#  - FULL_WAV_PATH   -> å®Œæ•´ä¼šè¯ WAVï¼ˆPCM16ï¼‰
#  - OUTPUT_JSON     -> ç¦»çº¿åˆå¹¶åæœ€ç»ˆ segments JSON
#  - DEBUG_RAW_JSON  -> ç¦»çº¿æ¨¡å‹åŸå§‹ sentence_infoï¼ˆè°ƒè¯•ç”¨ï¼‰
#
# å…³é”®é…ç½®ä¸ä¾èµ–ï¼ˆç¯å¢ƒå˜é‡ / å¸¸é‡ï¼‰ï¼š
#  - é‡‡æ ·ç‡ RATE = 16000, CHANNELS = 1
#  - å®æ—¶æ¨¡å‹ï¼šRT_ASR_MODEL, RT_VAD_MODELï¼ˆä½¿ç”¨ funasr.AutoModelï¼‰
#  - ç¦»çº¿æ¨¡å‹è·¯å¾„ï¼šé€šè¿‡ get_model_paths() æŒ‡å‘æœ¬åœ° modelscope ç¼“å­˜
#  - DeepSeekï¼šUSE_LOCAL_DEEPSEEK (Ollama æœ¬åœ°) æˆ– å®˜æ–¹äº‘ç«¯ï¼ˆDEEPSEEK_API_KEY / DEEPSEEK_BASE_URLï¼‰
#  - éœ€è¦ä¾èµ–ï¼špython åŒ… numpy, ffmpeg-python, requests, funasr, fastapi, uvicorn ç­‰ï¼›ç³»ç»Ÿéœ€å®‰è£… ffmpeg
#
# è¿è¡Œæ–¹å¼ï¼š
#  - ç›´æ¥è¿è¡Œï¼špython `app.py`ï¼Œå¯é€‰å‚æ•° --host/--portï¼ˆé»˜è®¤ 0.0.0.0:27000ï¼‰
#
# æ³¨æ„äº‹é¡¹ï¼š
#  - ä¸ºé¿å…æ¯æ¬¡è¿æ¥é‡å¤åŠ è½½ï¼Œå®æ—¶æ¨¡å‹åœ¨æ¨¡å—çº§åˆ«å…¨å±€åŠ è½½ã€‚
#  - ç¦»çº¿å¤„ç†ä¼šåœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œä»¥é¿å…é˜»å¡äº‹ä»¶å¾ªç¯ã€‚
#  - è¯¥æ–‡ä»¶åŒæ—¶åŒ…å«å®æ—¶ WebSocket æœåŠ¡ä¸ç¦»çº¿åå¤„ç†é€»è¾‘ï¼ˆVAD/ASR/PUNC/spk + DeepSeekï¼‰ã€‚
# =========================================================

from datetime import datetime
import os
import json
import wave
import time
import asyncio
from typing import List, Dict, Any
from urllib.parse import parse_qs
import argparse

import numpy as np
import ffmpeg
import requests
from funasr import AutoModel

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, Request, HTTPException
from fastapi.exceptions import RequestValidationError
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from starlette.status import HTTP_422_UNPROCESSABLE_ENTITY
from pydantic import BaseModel
import uvicorn

# åŠ è½½æœ¬åœ°ç¯å¢ƒå˜é‡ï¼ˆä¸»è¦æ˜¯DEEPSEEK_API_KEYï¼‰
from dotenv import load_dotenv

load_dotenv()


# =========================================================
# è¾“å‡ºä¼šè¯ç›®å½•ï¼ˆæŒ‰æ—¶é—´æˆ³ï¼‰
# =========================================================
def make_output_session(base_dir="output"):
	now = datetime.now()
	session_name = now.strftime("%Y%m%d_%H%M%S")
	session_dir = os.path.join(base_dir, session_name)
	
	os.makedirs(session_dir, exist_ok=True)
	
	paths = {
		"base" : session_dir,
		"wav"  : os.path.join(session_dir, "session_full.wav"),
		"json" : os.path.join(session_dir, "stream_output.json"),
		"debug": os.path.join(session_dir, "debug_segments_raw.json"),
	}
	return paths


# æ¯æ¬¡ WebSocket è¿æ¥ï¼ˆä¸€æ¬¡å½•éŸ³ï¼‰éƒ½è¦é‡æ–°ç”Ÿæˆ session ç›®å½•
paths = None

FULL_WAV_PATH = ""
OUTPUT_JSON = ""
DEBUG_RAW_JSON = ""

# =========================================================
# éŸ³é¢‘å‚æ•°ï¼ˆç½‘é¡µä¾§å¿…é¡»åŒ¹é…ï¼‰
# =========================================================
RATE = 16000
CHANNELS = 1  # mono

# =========================================================
# å®æ—¶å‚æ•°
# =========================================================
RT_VAD_CHUNK_MS = 300
RT_MAX_END_SILENCE_MS = 500

RT_ASR_MODEL = "iic/SenseVoiceSmall"
RT_VAD_MODEL = "fsmn-vad"
RT_VAD_REV = "v2.0.4"

ASR_DEVICE = "cuda:0"
VAD_DEVICE = "cuda:0"

# =========================================================
# ç¦»çº¿è¾“å‡ºåˆå¹¶è§„åˆ™ï¼ˆåŒ spk ä¸” gap <= MERGE_GAP_MSï¼‰
# =========================================================
MERGE_CONTINUOUS_SPK = True
MERGE_GAP_MS = 300

# =========================================================
# DeepSeek æ€»ç»“é…ç½®ï¼ˆæ”¯æŒ Ollama æœ¬åœ° / å®˜æ–¹äº‘ç«¯ï¼‰
# =========================================================
USE_LOCAL_DEEPSEEK = False  # True = Ollama æœ¬åœ°ï¼›False = å®˜æ–¹ DeepSeek API

# ---- Local Ollama DeepSeek ----
OLLAMA_DEEPSEEK_URL = os.getenv(
	"OLLAMA_DEEPSEEK_URL",
	"http://192.168.10.90:11434/api/generate"
)
OLLAMA_DEEPSEEK_MODEL = os.getenv(
	"OLLAMA_DEEPSEEK_MODEL",
	"deepseek-r1:32b"
)

# ---- Cloud DeepSeek ----
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "").strip()
DEEPSEEK_BASE_URL = os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com").rstrip("/")
DEEPSEEK_MODEL = os.getenv("DEEPSEEK_MODEL", "deepseek-chat").strip()

if not DEEPSEEK_API_KEY or DEEPSEEK_API_KEY == "sk-xxx":
	raise RuntimeError("DEEPSEEK_API_KEY is not set")


def _deepseek_summarize_ollama(raw_text: str) -> str:
	if not raw_text.strip():
		return ""
	
	prompt = (
			"ä½ æ˜¯ä¸€ä¸ªä¼šè®®çºªè¦åŠ©æ‰‹ï¼Œç”Ÿæˆä¼šè®®çºªè¦ã€‚æ³¨æ„ï¼šä¸è¦ä½¿ç”¨mdæ ¼å¼ï¼Œåªè¾“å‡ºçº¯æ–‡æœ¬ã€‚"
			+ "ä¼šè®®ç»“æŸçš„æ—¶é—´ä¸ºï¼š" + datetime.now().strftime("%Y-%m-%d %H:%M:%S")
			+ raw_text
	)
	
	payload = {
		"model" : OLLAMA_DEEPSEEK_MODEL,
		"prompt": prompt,
		"stream": False,
	}
	
	try:
		resp = requests.post(
			OLLAMA_DEEPSEEK_URL,
			headers={"Content-Type": "application/json"},
			json=payload,
			timeout=120,
		)
		resp.raise_for_status()
		return resp.json().get("response", "").strip()
	except Exception as e:
		print(f"[DeepSeek Ollama Error] {e}")
		return ""


def _deepseek_summarize_cloud(raw_text: str) -> str:
	if not raw_text.strip() or not DEEPSEEK_API_KEY:
		return ""
	
	payload = {
		"model"      : DEEPSEEK_MODEL,
		"messages"   : [
			{"role"   : "system",
			 "content": "ä½ æ˜¯ä¸€ä¸ªä¼šè®®çºªè¦åŠ©æ‰‹ï¼Œç”Ÿæˆä¼šè®®çºªè¦ã€‚æ³¨æ„ï¼šä¸è¦ä½¿ç”¨mdæ ¼å¼ï¼Œåªè¾“å‡ºçº¯æ–‡æœ¬ã€‚" +
			            "ä¼šè®®ç»“æŸçš„æ—¶é—´ä¸ºï¼š" + datetime.now().strftime("%Y-%m-%d %H:%M:%S")},
			{"role": "user", "content": raw_text},
		],
		"temperature": 0.2,
	}
	
	try:
		resp = requests.post(
			f"{DEEPSEEK_BASE_URL}/chat/completions",
			headers={
				"Authorization": f"Bearer {DEEPSEEK_API_KEY}",
				"Content-Type" : "application/json",
			},
			json=payload,
			timeout=60,
		)
		resp.raise_for_status()
		j = resp.json()
		return j["choices"][0]["message"]["content"].strip()
	except Exception as e:
		print(f"[DeepSeek Cloud Error] {e}")
		return ""


def deepseek_summarize(raw_text: str) -> str:
	if USE_LOCAL_DEEPSEEK:
		return _deepseek_summarize_ollama(raw_text)
	else:
		return _deepseek_summarize_cloud(raw_text)


# =========================================================
# å·¥å…·ï¼šç¡®ä¿ç›®å½•ã€WAV å†™å…¥ï¼ˆPCM16ï¼‰
# =========================================================
def ensure_dir_for_file(path: str):
	d = os.path.dirname(path)
	if d:
		os.makedirs(d, exist_ok=True)


def save_wav_pcm16(path: str, audio_i16: np.ndarray, sr: int = 16000):
	ensure_dir_for_file(path)
	with wave.open(path, "wb") as wf:
		wf.setnchannels(1)
		wf.setsampwidth(2)  # int16
		wf.setframerate(sr)
		wf.writeframes(audio_i16.tobytes())


# =========================================================
# ç¦»çº¿é˜¶æ®µ
#   1) ffmpeg è¯»éŸ³é¢‘ä¸º wav bytes(16k mono pcm_s16le)
#   2) AutoModel(asr+vad+punc+spk) generate(sentence_timestamp=True)
#   3) sentence_info -> build_output_units(merge gap)
# =========================================================
def get_model_paths() -> Dict[str, str]:
	home = os.path.expanduser("~")
	base = os.path.join(home, ".cache", "modelscope", "hub", "models", "iic")
	return {
		"asr" : os.path.join(base, "speech_seaco_paraformer_large_asr_nat-zh-cn-16k-common-vocab8404-pytorch"),
		"vad" : os.path.join(base, "speech_fsmn_vad_zh-cn-16k-common-pytorch"),
		"punc": os.path.join(base, "punc_ct-transformer_zh-cn-common-vocab272727-pytorch"),
		"spk" : os.path.join(base, "speech_campplus_sv_zh-cn_16k-common"),
	}


def load_audio_bytes(audio_path: str) -> bytes:
	audio_bytes, _ = (
		ffmpeg
		.input(audio_path, threads=0)
		.output("-", format="wav", acodec="pcm_s16le", ac=1, ar=16000)
		.run(cmd=["ffmpeg", "-nostdin"], capture_stdout=True, capture_stderr=True)
	)
	return audio_bytes


def load_offline_model(device: str = "cuda", ngpu: int = 1, ncpu: int = 4) -> AutoModel:
	p = get_model_paths()
	return AutoModel(
		model=p["asr"],
		vad_model=p["vad"],
		punc_model=p["punc"],
		spk_model=p["spk"],
		ngpu=ngpu,
		ncpu=ncpu,
		device=device,
		disable_pbar=True,
		disable_log=True,
		disable_update=True,
	)


def run_offline_asr(model: AutoModel, audio_bytes: bytes) -> Dict[str, Any]:
	res = model.generate(
		input=audio_bytes,
		batch_size_s=300,
		is_final=True,
		sentence_timestamp=True,
	)
	return res[0] if res else {}


def build_output_units(sentence_info: List[Dict[str, Any]],
                       merge_continuous_spk: bool,
                       merge_gap_ms: int) -> List[Dict[str, Any]]:
	units: List[Dict[str, Any]] = []
	for s in sentence_info or []:
		text = (s.get("text") or "").strip()
		if not text:
			continue
		
		spk = s.get("spk")
		start = int(s.get("start", 0))
		end = int(s.get("end", 0))
		
		if merge_continuous_spk and units:
			last = units[-1]
			same_spk = (last["spk"] == spk)
			gap = start - last["end_ms"]
			if same_spk and gap <= merge_gap_ms:
				last["text"] += text
				last["end_ms"] = end
				continue
		
		units.append({
			"spk"     : spk,
			"text"    : text,
			"start_ms": start,
			"end_ms"  : end,
		})
	return units


def save_json(obj: Dict[str, Any], path: str):
	ensure_dir_for_file(path)
	with open(path, "w", encoding="utf-8") as f:
		json.dump(obj, f, ensure_ascii=False, indent=2)


def offline_postprocess(full_wav_path: str) -> Dict[str, Any]:
	"""
	è·‘ç¦»çº¿ï¼šVAD â†’ ASR â†’ PUNC â†’ Cam++ (spk) ï¼Œè½ç›˜ JSONï¼Œå¹¶è¿”å› out_obj
	"""
	print("\nğŸ§¾ Offline: VAD â†’ ASR â†’ PUNC â†’ Cam++ ...")
	audio_bytes = load_audio_bytes(full_wav_path)
	
	model = load_offline_model(device="cuda", ngpu=1, ncpu=4)
	rec = run_offline_asr(model, audio_bytes)
	
	# åŸå§‹ sentence_infoï¼ˆdebugï¼‰
	debug_obj = {
		"audio_full"   : {"path": FULL_WAV_PATH, "sample_rate": RATE, "channels": 1},
		"sentence_info": rec.get("sentence_info", []),
	}
	save_json(debug_obj, DEBUG_RAW_JSON)
	
	# åˆå¹¶åçš„ segmentsï¼ˆæœ€ç»ˆè¾“å‡ºï¼‰
	units = build_output_units(
		rec.get("sentence_info", []),
		MERGE_CONTINUOUS_SPK,
		MERGE_GAP_MS,
	)
	
	out_obj = {
		"audio_full": {"path": FULL_WAV_PATH, "sample_rate": RATE, "channels": 1},
		"segments"  : units,
	}
	save_json(out_obj, OUTPUT_JSON)
	
	print(f"âœ… Saved:\n  - {FULL_WAV_PATH}\n  - {OUTPUT_JSON}\n  - {DEBUG_RAW_JSON}")
	return out_obj


def build_transcript_for_summary(result_obj: Dict[str, Any]) -> str:
	"""
	æŠŠç¦»çº¿ segments æ‹¼æˆé€‚åˆæ€»ç»“çš„å…¨æ–‡ï¼ˆå¸¦ speaker/æ—¶é—´æˆ³ï¼‰
	"""
	segs = result_obj.get("segments") or []
	lines = []
	for s in segs:
		spk = s.get("spk", "spk")
		t0 = int(s.get("start_ms", 0))
		t1 = int(s.get("end_ms", 0))
		txt = (s.get("text") or "").strip()
		if not txt:
			continue
		lines.append(f"[{t0}-{t1}] {spk}: {txt}")
	return "\n".join(lines).strip()


# =========================================================
# FastAPI / WebSocket
# =========================================================
# code è¯­ä¹‰å®šä¹‰ï¼š
# code = 0  ã€å®æ—¶è¯†åˆ«åˆ†æ®µç»“æœ / partial segmentã€‘
#   - è§¦å‘æ—¶æœºï¼š
#       æ¯ä¸€æ¬¡ VAD ç»“æŸ + ASR äº§å‡ºæ–‡æœ¬
#   - info:
#       JSON å­—ç¬¦ä¸²ï¼ŒåŒ…å«æ—¶é—´ä¸è€—æ—¶ä¿¡æ¯
#   - data:
#       è¯¥åˆ†æ®µå¯¹åº”çš„è¯†åˆ«æ–‡æœ¬
# -------------------------
# code = 1  ã€æœ€ç»ˆå­—å¹•è¯†åˆ«ç»“æœ / final resultã€‘
#   - è§¦å‘æ—¶æœºï¼š
#       WebSocket å³å°†ç»“æŸå‰ï¼Œæ•´æ®µéŸ³é¢‘ç¦»çº¿å¤„ç†å®Œæˆ
#   - info:
#       å›ºå®šå­—ç¬¦ä¸²ï¼š"final"
#   - data:
#       final_obj çš„ JSON å­—ç¬¦ä¸²ï¼ˆensure_ascii=Falseï¼‰
#       ç»“æ„ç¤ºä¾‹ï¼š
#       {
#         "audio_full": {...},
#         "segments": [
#           {
#             "spk": <speaker_id>,
#             "text": "...",
#             "start_ms": ...,
#             "end_ms": ...
#           }
#         ]
#       }
#
# -------------------------
# code = 2  ã€çŠ¶æ€ / äº‹ä»¶é€šçŸ¥ï¼ˆé ASR æ–‡æœ¬ï¼‰ã€‘
#   - è§¦å‘æ—¶æœºï¼š
#       WebSocket ç»“æŸå¹¶å›ä¼ æœ€åçš„aiæ€»ç»“
#   - info:
#       å›ºå®šå­—ç¬¦ä¸²ï¼š"summary"
#   data:
#     - summary           : AIæ€»ç»“çš„summary æ–‡æœ¬
#
# =========================================================
class TranscriptionResponse(BaseModel):
	code: int  # æ¶ˆæ¯ç±»å‹ / çŠ¶æ€ç 
	info: str = ""  # å…ƒä¿¡æ¯ï¼ˆJSON å­—ç¬¦ä¸² / æ ‡è®°å­—ç¬¦ä¸²ï¼‰
	data: str = ""  # å®é™…è½½è·ï¼ˆæ–‡æœ¬ / JSON å­—ç¬¦ä¸²ï¼‰


app = FastAPI()
app.add_middleware(
	CORSMiddleware,
	allow_origins=["*"],
	allow_credentials=True,
	allow_methods=["*"],
	allow_headers=["*"],
)


@app.exception_handler(Exception)
async def custom_exception_handler(request: Request, exc: Exception):
	if isinstance(exc, HTTPException):
		status_code = exc.status_code
		message = str(exc.detail)
	elif isinstance(exc, RequestValidationError):
		status_code = HTTP_422_UNPROCESSABLE_ENTITY
		message = "Validation error: " + str(exc.errors())
	else:
		status_code = 500
		message = "Internal server error: " + str(exc)
	return JSONResponse(
		status_code=status_code,
		content=TranscriptionResponse(code=status_code, info=message, data="").model_dump(),
	)


@app.get("/health")
async def health():
	return {"ok": True}


# =========================================================
# å…¨å±€åŠ è½½å®æ—¶æ¨¡å‹ï¼ˆé¿å…æ¯ä¸ªWSè¿æ¥éƒ½åŠ è½½ä¸€æ¬¡ï¼‰
# =========================================================
rt_vad = AutoModel(
	model=RT_VAD_MODEL,
	model_revision=RT_VAD_REV,
	disable_pbar=True,
	max_end_silence_time=RT_MAX_END_SILENCE_MS,
	device=VAD_DEVICE,
	disable_update=True,
)

rt_asr = AutoModel(
	model=RT_ASR_MODEL,
	trust_remote_code=True,
	device=ASR_DEVICE,
	disable_update=True,
)


# =========================================================
# WebSocket: /ws/transcribe
#  - æ”¶åˆ°éŸ³é¢‘æµï¼šå®æ—¶ VADâ†’ASR å›ä¼ 
#  - æ–­å¼€æ—¶ï¼šä¿å­˜ FULL_WAV_PATHï¼Œè·‘ offline_postprocessï¼Œå†æŠŠæœ€ç»ˆ JSON + deepseek summary å›ä¼ ï¼ˆå¦‚æœè¿˜èƒ½å‘ï¼‰
# =========================================================
@app.websocket("/ws/transcribe")
async def ws_transcribe(websocket: WebSocket):
	"""
	Query:
	  - lang=auto (default auto)
	  - sv=0/1 (ç›®å‰æµç¨‹Aé‡Œå…ˆä¸å½±å“é€»è¾‘ï¼Œä½ åé¢è¦ç”¨å†æ‰©å±•)
	Stream:
	  - binary PCM16LE, 16kHz, mono
	  - text/json: {"type":"end"} æˆ– "end" è¡¨ç¤ºå½•éŸ³ç»“æŸï¼ˆæµç¨‹Aå…³é”®ï¼‰
	"""
	# æ¯æ¬¡è¿æ¥éƒ½åˆ›å»ºæ–°çš„ä¼šè¯ç›®å½•ï¼ˆä¸€æ¬¡å½•éŸ³ä¸€æ¬¡ç›®å½•ï¼‰
	local_paths = make_output_session("output")
	local_full_wav = local_paths["wav"]
	local_output_json = local_paths["json"]
	local_debug_json = local_paths["debug"]
	
	query_params = parse_qs(websocket.scope.get("query_string", b"").decode(errors="ignore"))
	lang = (query_params.get("lang", ["auto"])[0] or "auto").strip()
	
	await websocket.accept()
	print(f"âœ… WS connected. lang={lang} session={local_paths['base']}")
	
	chunk_size = int(RT_VAD_CHUNK_MS * RATE / 1000)
	
	# bytes å¯¹é½ç¼“å†²
	buf_bytes = b""
	
	# float32 ç¼“å†²
	audio_buffer = np.array([], dtype=np.float32)
	audio_vad = np.array([], dtype=np.float32)
	
	# ä¿å­˜å…¨é‡ int16ï¼ˆç”¨äºæœ€ç»ˆç¦»çº¿ï¼‰
	full_i16_chunks: List[np.ndarray] = []
	
	cache_vad: Dict[str, Any] = {}
	cache_asr: Dict[str, Any] = {}
	
	last_vad_beg = -1
	last_vad_end = -1
	offset_ms = 0
	
	ensure_dir_for_file(local_full_wav)
	
	# æµç¨‹Aï¼šé€šè¿‡â€œendâ€ä¿¡å·ç»“æŸ while å¾ªç¯ï¼Œè€Œä¸æ˜¯é æ–­å¼€
	ended_by_client = False
	
	try:
		while True:
			msg = await websocket.receive()
			
			# 1) å‰ç«¯å‘é€äºŒè¿›åˆ¶éŸ³é¢‘
			if "bytes" in msg and msg["bytes"] is not None:
				data = msg["bytes"]
				if not data:
					continue
				
				buf_bytes += data
				if len(buf_bytes) < 2:
					continue
				
				aligned_len = len(buf_bytes) - (len(buf_bytes) % 2)
				if aligned_len <= 0:
					continue
				
				audio_i16 = np.frombuffer(buf_bytes[:aligned_len], dtype=np.int16)
				buf_bytes = buf_bytes[aligned_len:]
				
				if audio_i16.size == 0:
					continue
				
				full_i16_chunks.append(audio_i16.copy())
				
				audio_f32 = audio_i16.astype(np.float32) / 32767.0
				audio_buffer = np.append(audio_buffer, audio_f32)
				
				while len(audio_buffer) >= chunk_size:
					chunk = audio_buffer[:chunk_size]
					audio_buffer = audio_buffer[chunk_size:]
					audio_vad = np.append(audio_vad, chunk)
					
					res_vad = rt_vad.generate(
						input=chunk,
						cache=cache_vad,
						is_final=False,
						chunk_size=RT_VAD_CHUNK_MS,
					)
					
					values = []
					if res_vad and isinstance(res_vad, list) and res_vad[0].get("value") is not None:
						values = res_vad[0]["value"]
					if not values:
						continue
					
					for seg in values:
						if seg[0] > -1:
							last_vad_beg = seg[0]
						if seg[1] > -1:
							last_vad_end = seg[1]
						
						if last_vad_beg > -1 and last_vad_end > -1:
							beg_ms_local = last_vad_beg - offset_ms
							end_ms_local = last_vad_end - offset_ms
							offset_ms += end_ms_local
							
							beg = int(beg_ms_local * RATE / 1000)
							end = int(end_ms_local * RATE / 1000)
							
							beg = max(beg, 0)
							end = min(end, len(audio_vad))
							
							speech = audio_vad[beg:end]
							
							t0 = time.time()
							asr_out = rt_asr.generate(
								input=speech,
								cache=cache_asr,
								language=lang,
								use_itn=True,
							)
							t1 = time.time()
							
							text = ""
							if asr_out and isinstance(asr_out, list):
								text = (asr_out[0].get("text") or "").strip()
							
							seg_end_ms_global = int(offset_ms)
							seg_start_ms_global = int(offset_ms - end_ms_local)
							
							if text:
								await websocket.send_json(
									TranscriptionResponse(
										code=0,
										info=json.dumps(
											{
												"start_ms": seg_start_ms_global,
												"end_ms"  : seg_end_ms_global,
												"asr_ms"  : int((t1 - t0) * 1000),
											},
											ensure_ascii=False,
										),
										data=text,
									).model_dump()
								)
							
							# æ¶ˆè´¹å·²ç”¨éŸ³é¢‘
							audio_vad = audio_vad[end:]
							last_vad_beg = -1
							last_vad_end = -1
				
				continue
			
			# 2) å‰ç«¯å‘é€æ–‡æœ¬ï¼ˆç»“æŸä¿¡å·ï¼‰
			if "text" in msg and msg["text"] is not None:
				txt = (msg["text"] or "").strip()
				if not txt:
					continue
				
				# æ”¯æŒä¸¤ç§ï¼šçº¯å­—ç¬¦ä¸² "end" / JSON {"type":"end"}
				is_end = False
				if txt.lower() in ("end", "stop", "finish", "done"):
					is_end = True
				else:
					try:
						j = json.loads(txt)
						if isinstance(j, dict) and str(j.get("type", "")).lower() in ("end", "stop", "finish", "done"):
							is_end = True
					except Exception:
						pass
				
				if is_end:
					print("ğŸŸ¦ Received end signal from client. Start video_processor postprocess...")
					ended_by_client = True
					break
	
	except WebSocketDisconnect:
		print("ğŸ”Œ WS disconnected (client closed before end).")
	except Exception as e:
		print(f"âŒ WS error: {e}")
		try:
			await websocket.close()
		except Exception:
			pass
		return
	
	# =========================
	# è¿™é‡Œå¼€å§‹â€œå½•éŸ³ç»“æŸåçš„ç¦»çº¿æµç¨‹â€ï¼Œä½† WS ä»ç„¶ä¿æŒæ‰“å¼€
	# =========================
	try:
		# 1) ä¿å­˜å®Œæ•´ wav
		full_i16 = np.concatenate(full_i16_chunks, axis=0) if full_i16_chunks else np.zeros(0, dtype=np.int16)
		save_wav_pcm16(local_full_wav, full_i16, RATE)
		print(f"âœ… Full audio saved: {local_full_wav} (samples={len(full_i16)})")
		
		# 2) ç¦»çº¿ spk è¾“å‡ºï¼ˆçº¿ç¨‹é¿å…å¡ event loopï¼‰
		def _offline_postprocess_local(full_wav_path: str) -> Dict[str, Any]:
			# ä¸´æ—¶è¦†ç›–å…¨å±€è¾“å‡ºè·¯å¾„ï¼Œè®© offline_postprocess è½åˆ°æœ¬æ¬¡ session ç›®å½•
			global FULL_WAV_PATH, OUTPUT_JSON, DEBUG_RAW_JSON
			FULL_WAV_PATH = local_full_wav
			OUTPUT_JSON = local_output_json
			DEBUG_RAW_JSON = local_debug_json
			return offline_postprocess(full_wav_path)
		
		final_obj: Dict[str, Any] = await asyncio.to_thread(_offline_postprocess_local, local_full_wav)
		
		# 3) DeepSeek æ€»ç»“
		try:
			transcript = build_transcript_for_summary(final_obj)
			print("è¿™æ˜¯ç”¨äº DeepSeek æ€»ç»“çš„ transcript å†…å®¹é¢„è§ˆï¼š")
			print("-----")
			print(transcript)
			print("-----")
			summary_text = await asyncio.to_thread(deepseek_summarize, transcript)
			print("âœ… DeepSeek summary generated.")
		except Exception as e:
			print(f"âŒ deepseek summarize failed: {e}")
			summary_text = ""
		# summary_text = "è¿™æ˜¯æµ‹è¯•ç”¨çš„summaryå ä½ç¬¦"
		
		# 4) å›ä¼ æœ€ç»ˆ JSON (code=1)
		try:
			await websocket.send_json(
				TranscriptionResponse(
					code=1,
					info="final",
					data=json.dumps(final_obj, ensure_ascii=False),
				).model_dump()
			)
		except Exception as e:
			print(f"âš ï¸ send final failed (maybe client already closed): {e}")
		
		# 5) å›ä¼  summary (code=2)
		try:
			await websocket.send_json(
				TranscriptionResponse(
					code=2,
					info="summary",
					data=summary_text,
				).model_dump()
			)
		except Exception as e:
			print(f"âš ï¸ send final failed (maybe client already closed): {e}")
	
	finally:
		# æœåŠ¡ç«¯ä¸»åŠ¨å…³é—­ï¼ˆå‰ç«¯ä¹Ÿä¼šåœ¨æ”¶åˆ° code=1 å closeï¼‰
		try:
			await websocket.close()
		except Exception:
			pass


if __name__ == "__main__":
	parser = argparse.ArgumentParser(
		description="Run RT(WebSocket) + Offline(speaker clustering) + DeepSeek summary server.")
	parser.add_argument("--host", type=str, default="0.0.0.0")
	parser.add_argument("--port", type=int, default=27000)
	args = parser.parse_args()
	uvicorn.run(app, host=args.host, port=args.port)
