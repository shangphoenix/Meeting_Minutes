# spk_RT&offline_cam++.py
# è¿™ä¸ªçš„é—®é¢˜æ˜¯:
# ç»“æŸä¹‹åæ²¡æœ‰å†è·‘ä¸€æ¬¡ASR+VAD,å¯¼è‡´CAM++æ— æ³•è¯†åˆ«ä¸åŒè¯´è¯äºº
import json
import os
import re
import time
import math
import wave
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional

import numpy as np
import pyaudio
import torch
from funasr import AutoModel

# =========================
# 0) å‚æ•°
# =========================
RATE = 16000
CHANNELS = 1
FORMAT = pyaudio.paInt16
CHUNK = 1024

# VAD æµå¼ chunk å¤§å°ï¼ˆmsï¼‰
CHUNK_MS = 300
MAX_END_SILENCE_MS = 500

# ç¦»çº¿å£°çº¹å‰ï¼Œå…ˆæŠŠç¢æ®µåˆå¹¶æˆé•¿å—
SPK_PREMERGE_GAP_MS = 800  # ä¸¤æ®µä¹‹é—´ <= 800ms å°±å…ˆæ‹¼èµ·æ¥ç®—å£°çº¹
SPK_MIN_CHUNK_MS = 2500  # æ‹¼å‡ºæ¥çš„å—é•¿åº¦ < 2.5s çš„å£°çº¹å¾ˆä¸ç¨³ï¼ˆå¯è°ƒï¼‰
SPK_MIN_EMB_MS = 1200  # æœ€ä½ 1.2s æ‰å°è¯•ç®— embï¼ˆå…œåº•ï¼‰

# æœ€ç»ˆè¾“å‡ºå†æŒ‰åŒ spk åˆå¹¶
FINAL_MERGE_GAP_MS = 300  # åŒ spk ä¸” gap <= 300msï¼Œæ‹¼æ–‡æœ¬
FINAL_MERGE_CONTINUOUS_SPK = True

# è¯´è¯äººåœ¨çº¿èšç±»é˜ˆå€¼ï¼ˆè¶Šå¤§è¶Šä¸å®¹æ˜“æŠŠä¸åŒäººåˆå¹¶ï¼‰
SPK_COS_THRESH = 0.80

# è¾“å‡º
OUTPUT_JSON_PATH = "output/realtime_captions.json"
OUTPUT_AUDIO_PATH = "output/realtime_captions.wav"

# è®¾å¤‡ï¼ˆæŒ‰éœ€æ”¹ï¼‰
ASR_DEVICE = "cuda:0"
VAD_DEVICE = "cuda:0"
SPK_DEVICE = "cuda:0"

# =========================
# 1) å®æ—¶æ¨¡å‹ï¼ˆåªåŠ è½½ ASR + VADï¼‰
# =========================
model_asr = AutoModel(
	model="iic/SenseVoiceSmall",
	trust_remote_code=True,
	device=ASR_DEVICE,
	disable_update=True
)

model_vad = AutoModel(
	model="fsmn-vad",
	model_revision="v2.0.4",
	disable_pbar=True,
	max_end_silence_time=MAX_END_SILENCE_MS,
	disable_update=True
)

# =========================
# 2) ç¦»çº¿å£°çº¹ï¼šCam++ æ¨¡å‹ï¼ˆåœæ­¢åæ‰åŠ è½½/è°ƒç”¨ï¼‰
# =========================
SPK_MODEL_NAME = "iic/speech_campplus_sv_zh-cn_16k-common"


# =========================
# 3) Online clusteringï¼šassign_spk
# =========================
def _l2norm(x: np.ndarray, eps: float = 1e-12) -> np.ndarray:
	n = float(np.linalg.norm(x)) + eps
	return x / n


def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:
	a = _l2norm(a)
	b = _l2norm(b)
	return float(np.dot(a, b))


@dataclass
class SpeakerAssigner:
	cos_thresh: float = SPK_COS_THRESH
	centroids: List[np.ndarray] = field(default_factory=list)
	counts: List[int] = field(default_factory=list)
	
	def assign_spk(self, emb: np.ndarray) -> int:
		if emb is None:
			return -1
		emb = np.asarray(emb, dtype=np.float32).reshape(-1)
		emb = _l2norm(emb)
		
		if not self.centroids:
			self.centroids.append(emb)
			self.counts.append(1)
			return 0
		
		sims = [cosine_sim(emb, c) for c in self.centroids]
		best_i = int(np.argmax(sims))
		best_s = sims[best_i]
		
		if best_s >= self.cos_thresh:
			k = self.counts[best_i]
			new_c = (self.centroids[best_i] * k + emb) / (k + 1.0)
			self.centroids[best_i] = _l2norm(new_c)
			self.counts[best_i] = k + 1
			return best_i
		
		self.centroids.append(emb)
		self.counts.append(1)
		return len(self.centroids) - 1


# =========================
# 4) Cam++ embedding æå–ï¼ˆæ”¯æŒ CUDA tensorï¼‰
# =========================
def _to_numpy(x) -> Optional[np.ndarray]:
	if x is None:
		return None
	if torch.is_tensor(x):
		return x.detach().float().cpu().numpy().reshape(-1).astype(np.float32)
	arr = np.asarray(x, dtype=np.float32)
	return arr.reshape(-1)


def extract_spk_embedding(model: AutoModel, speech_f32: np.ndarray) -> Optional[np.ndarray]:
	"""
	å°½é‡å…¼å®¹ä¸åŒ funasr/campplus è¾“å‡ºç»“æ„ï¼Œå¹¶å®‰å…¨è½¬ numpy
	"""
	try:
		out = model.generate(input=speech_f32)
		if not out:
			return None
		obj = out[0] if isinstance(out, list) else out
		
		candidate_keys = [
			"spk_embedding", "speaker_embedding", "embedding", "emb",
			"spk_emb", "xvector", "vector"
		]
		
		# 1) ç›´æ¥ tensor/list/ndarray
		if torch.is_tensor(obj) or isinstance(obj, (list, tuple, np.ndarray)):
			return _to_numpy(obj)
		
		# 2) dict
		if isinstance(obj, dict):
			for k in candidate_keys:
				if k in obj and obj[k] is not None:
					return _to_numpy(obj[k])
			
			for nest_k in ["value", "outputs", "result", "data"]:
				v = obj.get(nest_k)
				if isinstance(v, dict):
					for k in candidate_keys:
						if k in v and v[k] is not None:
							return _to_numpy(v[k])
				elif v is not None and (torch.is_tensor(v) or isinstance(v, (list, tuple, np.ndarray))):
					arr = _to_numpy(v)
					if arr is not None and 8 <= arr.size <= 4096:
						return arr
			
			# å…œåº•ï¼šéå† values æ‰¾â€œåƒ embeddingâ€çš„å‘é‡
			for v in obj.values():
				if v is None:
					continue
				if torch.is_tensor(v) or isinstance(v, (list, tuple, np.ndarray)):
					arr = _to_numpy(v)
					if arr is not None and arr.ndim == 1 and 8 <= arr.size <= 4096:
						return arr
		
		return None
	except Exception as e:
		print(f"[WARN] extract_spk_embedding failed: {e}")
		return None


# =========================
# 5) åˆå¹¶å·¥å…·ï¼šå…ˆæ‹¼é•¿å—ç”¨äºå£°çº¹ï¼Œå†æœ€ç»ˆæŒ‰ spk åˆå¹¶è¾“å‡º
# =========================
def remove_tags(text: str) -> str:
	"""
	ç®€å•å»æ ‡ç­¾ï¼ˆå¦‚ <noise>ã€[laughter] ç­‰ï¼‰
	"""
	return re.sub(r"[\[<][^]>]*[>\]]", "", text).strip()


def save_wav(path: str, audio_f32: np.ndarray, sr: int = 16000):
	audio_i16 = np.clip(audio_f32, -1.0, 1.0)
	audio_i16 = (audio_i16 * 32767.0).astype(np.int16)
	
	os.makedirs(os.path.dirname(path), exist_ok=True)
	
	with wave.open(path, "wb") as wf:
		wf.setnchannels(1)
		wf.setsampwidth(2)
		wf.setframerate(sr)
		wf.writeframes(audio_i16.tobytes())


def merge_for_speaker(segments_raw: List[Dict[str, Any]],
                      gap_ms: int = SPK_PREMERGE_GAP_MS) -> List[Dict[str, Any]]:
	"""
	æŠŠå®æ—¶ç¢æ®µå…ˆæ‹¼æˆæ›´é•¿çš„â€œå£°çº¹å—â€ï¼ŒCam++ æ›´ç¨³å®š
	"""
	merged = []
	cur = None
	
	for s in segments_raw:
		if cur is None:
			cur = {
				"start_ms": int(s["start_ms"]),
				"end_ms"  : int(s["end_ms"]),
				"text"    : (s.get("text") or "").strip(),
				"audio"   : s["audio"].astype(np.float32),
				"children": [s],  # ä¿ç•™æ˜ å°„å…³ç³»ï¼ˆå¯é€‰ï¼‰
			}
			continue
		
		gap = int(s["start_ms"]) - int(cur["end_ms"])
		if gap <= gap_ms:
			cur["end_ms"] = int(s["end_ms"])
			cur["text"] = (cur["text"] + (s.get("text") or "")).strip()
			cur["audio"] = np.concatenate([cur["audio"], s["audio"].astype(np.float32)], axis=0)
			cur["children"].append(s)
		else:
			merged.append(cur)
			cur = {
				"start_ms": int(s["start_ms"]),
				"end_ms"  : int(s["end_ms"]),
				"text"    : (s.get("text") or "").strip(),
				"audio"   : s["audio"].astype(np.float32),
				"children": [s],
			}
	
	if cur is not None:
		merged.append(cur)
	
	return merged


def final_merge_by_spk(chunks_with_spk: List[Dict[str, Any]],
                       gap_ms: int = FINAL_MERGE_GAP_MS) -> List[Dict[str, Any]]:
	"""
	æœ€ç»ˆè¾“å‡ºåˆå¹¶ï¼šåŒ spk ä¸” gap å°å°±æ‹¼æ–‡æœ¬ã€æ‰© end
	"""
	out = []
	for c in chunks_with_spk:
		text = (c.get("text") or "").strip()
		if not text:
			continue
		spk = int(c.get("spk", -1))
		start_ms = int(c["start_ms"])
		end_ms = int(c["end_ms"])
		
		if FINAL_MERGE_CONTINUOUS_SPK and out:
			last = out[-1]
			if last["spk"] == spk and (start_ms - last["end_ms"]) <= gap_ms:
				last["text"] += text
				last["end_ms"] = end_ms
				continue
		
		out.append({
			"spk"     : spk,
			"text"    : text,
			"start_ms": start_ms,
			"end_ms"  : end_ms
		})
	return out


# =========================
# 6) ä¸»æµç¨‹ï¼šå®æ—¶ VAD+ASR å½•åˆ¶ç‰‡æ®µï¼›åœæ­¢åç¦»çº¿ Cam++
# =========================
def main():
	# Full audio buffer (for saving)
	full_audio = np.array([], dtype=np.float32)
	
	# PyAudio
	p = pyaudio.PyAudio()
	stream = p.open(
		format=FORMAT,
		channels=CHANNELS,
		rate=RATE,
		input=True,
		frames_per_buffer=CHUNK,
	)
	print("ğŸ™ï¸ Start streaming (Ctrl+C to stop). [Scheme A: NO real-time Cam++]")
	
	chunk_size = int(CHUNK_MS * RATE / 1000)
	
	audio_buffer = np.array([], dtype=np.float32)  # åŸå§‹é‡‡é›†ç¼“å­˜
	audio_vad = np.array([], dtype=np.float32)  # VAD æ—¶é—´è½´ç¼“å­˜
	
	cache_vad: Dict[str, Any] = {}
	cache_asr: Dict[str, Any] = {}
	
	last_vad_beg = -1
	last_vad_end = -1
	offset_ms = 0  # ç”¨äºæŠŠ VAD è¾“å‡ºæ˜ å°„æˆå½“å‰ audio_vad å±€éƒ¨åæ ‡ï¼ˆæ²¿ç”¨ä½ å®æ—¶è„šæœ¬çš„æ€è·¯ï¼‰
	
	segments_raw: List[Dict[str, Any]] = []
	
	try:
		while True:
			data = stream.read(CHUNK, exception_on_overflow=False)
			audio_i16 = np.frombuffer(data, dtype=np.int16)
			audio_f32 = audio_i16.astype(np.float32) / 32767.0
			audio_buffer = np.append(audio_buffer, audio_f32)
			full_audio = np.append(full_audio, audio_f32)
			
			while len(audio_buffer) >= chunk_size:
				chunk = audio_buffer[:chunk_size]
				audio_buffer = audio_buffer[chunk_size:]
				audio_vad = np.append(audio_vad, chunk)
				
				# ===== VAD streaming =====
				res_vad = model_vad.generate(
					input=chunk,
					cache=cache_vad,
					is_final=False,
					chunk_size=CHUNK_MS
				)
				
				values = []
				if res_vad and isinstance(res_vad, list) and res_vad[0].get("value") is not None:
					values = res_vad[0]["value"]
				
				if values:
					for seg in values:
						if seg[0] > -1:
							last_vad_beg = seg[0]
						if seg[1] > -1:
							last_vad_end = seg[1]
						
						if last_vad_beg > -1 and last_vad_end > -1:
							# seg æ˜¯ msï¼ˆç›¸å¯¹ç´¯è®¡æ—¶é—´è½´ï¼‰ï¼Œæ˜ å°„åˆ°å½“å‰ audio_vad
							beg_ms_local = last_vad_beg - offset_ms
							end_ms_local = last_vad_end - offset_ms
							
							# æ¶ˆè´¹åˆ° endï¼šæ›´æ–° offset_ms
							offset_ms += end_ms_local
							
							beg = int(beg_ms_local * RATE / 1000)
							end = int(end_ms_local * RATE / 1000)
							speech = audio_vad[beg:end]
							
							# ===== ASR =====
							asr_out = model_asr.generate(
								input=speech,
								cache=cache_asr,
								language="auto",
								use_itn=True
							)
							text = ""
							if asr_out and isinstance(asr_out, list):
								text = (asr_out[0].get("text") or "").strip()
							
							# å…¨å±€æ—¶é—´ï¼ˆmsï¼‰
							seg_end_ms_global = int(offset_ms)
							seg_start_ms_global = int(offset_ms - end_ms_local)
							
							segments_raw.append({
								"start_ms": seg_start_ms_global,
								"end_ms"  : seg_end_ms_global,
								"text"    : text,
								"audio"   : speech.astype(np.float32),
							})
							
							# å®æ—¶æ‰“å°
							if text:
								print(
									f"[{seg_start_ms_global / 1000:07.3f}-{seg_end_ms_global / 1000:07.3f}] {remove_tags(text)}")
								print(
									f"[{seg_start_ms_global / 1000:07.3f}-{seg_end_ms_global / 1000:07.3f}] {text}")
							
							# æ¸…ç†å·²æ¶ˆè´¹éŸ³é¢‘
							audio_vad = audio_vad[end:]
							last_vad_beg = -1
							last_vad_end = -1
	
	except KeyboardInterrupt:
		print("\nâ¹ï¸ stopped. Start video_processor Cam++ speaker assignment...")
	
	finally:
		stream.stop_stream()
		stream.close()
		p.terminate()
		save_wav(OUTPUT_AUDIO_PATH, full_audio, sr=RATE)
		print(f"âœ… saved full audio: {OUTPUT_AUDIO_PATH}")
	
	# =========================
	# 7) ç¦»çº¿é˜¶æ®µï¼šæ‹¼é•¿å— -> Cam++ -> èšç±» -> æœ€ç»ˆåˆå¹¶è¾“å‡º
	# =========================
	if not segments_raw:
		with open(OUTPUT_JSON_PATH, "w", encoding="utf-8") as f:
			json.dump({"segments": []}, f, ensure_ascii=False, indent=2)
		print(f"âœ… saved empty: {OUTPUT_JSON_PATH}")
		return
	
	# 7.1 å…ˆæ‹¼é•¿å—ï¼ˆé™ä½å£°çº¹è¯¯å·®ï¼‰
	spk_chunks = merge_for_speaker(segments_raw, gap_ms=SPK_PREMERGE_GAP_MS)
	
	# 7.2 åŠ è½½ Cam++ï¼ˆåªåœ¨ç¦»çº¿é˜¶æ®µï¼‰
	model_spk = AutoModel(
		model=SPK_MODEL_NAME,
		device=SPK_DEVICE,
		disable_update=True
	)
	
	assigner = SpeakerAssigner(cos_thresh=SPK_COS_THRESH)
	
	# 7.3 é€å—ç®— embedding å¹¶åˆ†é… spk
	for c in spk_chunks:
		dur_ms = int(len(c["audio"]) * 1000 / RATE)
		spk_id = -1
		
		# å¤ªçŸ­çš„å—ï¼šä¸ç»™ spkï¼ˆæˆ–ä½ ä¹Ÿå¯ä»¥é€‰æ‹©å¹¶åˆ°ç›¸é‚»å—ï¼‰
		if dur_ms >= SPK_MIN_EMB_MS and dur_ms >= SPK_MIN_CHUNK_MS:
			emb = extract_spk_embedding(model_spk, c["audio"])
			if emb is not None:
				spk_id = assigner.assign_spk(emb)
		
		c["spk"] = spk_id
	
	# 7.4 æœ€ç»ˆè¾“å‡ºå†æŒ‰åŒ spk åˆå¹¶ï¼ˆæ›´åƒä½ ç¦»çº¿è„šæœ¬çš„ç»“æœï¼‰
	final_segments = final_merge_by_spk(spk_chunks, gap_ms=FINAL_MERGE_GAP_MS)
	
	with open(OUTPUT_JSON_PATH, "w", encoding="utf-8") as f:
		json.dump({"segments": final_segments}, f, ensure_ascii=False, indent=2)
	
	print(
		f"âœ… saved: {OUTPUT_JSON_PATH} (raw={len(segments_raw)}, spk_chunks={len(spk_chunks)}, final={len(final_segments)})")


if __name__ == "__main__":
	main()
