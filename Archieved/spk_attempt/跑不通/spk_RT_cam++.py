# spk_RT_cam++.py
# è¿™ä¸ªæ–¹æ¡ˆæ˜¯å®æ—¶æµå¼é‡‡é›†éŸ³é¢‘ï¼Œç„¶åç”¨ VAD åˆ‡æ®µï¼Œåˆ‡å¥½çš„æ®µå†é€ ASR + SPK æ¨¡å‹æ¨ç†ï¼Œ
# æœ€ååœ¨çº¿åˆå¹¶è¾“å‡ºå•å…ƒå¹¶æ‰“å°ï¼ŒåŒæ—¶ä¿å­˜åˆ° JSON æ–‡ä»¶ã€‚
# è¯´è¯äººèšç±»ç”¨çš„æ˜¯ Cam++ / CAMPPlus æ¨¡å‹æå–çš„
# ä½†æ˜¯æœ‰ä¸€ä¸ªè¾ƒå¤§çš„é—®é¢˜:
# ç”±äº Cam++ / CAMPPlus æ¨¡å‹æœ¬èº«å¹¶ä¸æ˜¯ä¸ºå®æ—¶è®¾è®¡çš„ï¼Œ
# å®æ—¶è¯†åˆ«ä¼šå¯¼è‡´è¯´è¯äººåˆ†é…ä¸ç¨³å®šï¼Œå®¹æ˜“å‡ºç°åŒä¸€è¯´è¯äººè¢«åˆ†é…æˆå¤šä¸ª ID çš„æƒ…å†µã€‚

import json
import time
import numpy as np
import pyaudio
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Any

import torch
from funasr import AutoModel

# =========================
# 0) å‚æ•°
# =========================
RATE = 16000
CHANNELS = 1
FORMAT = pyaudio.paInt16
CHUNK = 1024  # PyAudio æ¯æ¬¡ read çš„å¸§æ•°
CHUNK_MS = 300  # VAD æµå¼ chunk å¤§å°ï¼ˆmsï¼‰
MAX_END_SILENCE_MS = 500  # ä½ åŸå®æ—¶è„šæœ¬é‡Œç”¨çš„

# åˆå¹¶è§„åˆ™ï¼ˆæ¥è‡ªç¦»çº¿è„šæœ¬ï¼‰
MERGE_CONTINUOUS_SPK = True
MERGE_GAP_MS = 300

# è¯´è¯äººåœ¨çº¿èšç±»é˜ˆå€¼ï¼ˆç»éªŒå€¼ï¼šè¶Šå¤§è¶Šâ€œä¿å®ˆâ€æ›´å®¹æ˜“æ–°å»ºè¯´è¯äººï¼‰
SPK_COS_THRESH = 0.65
SPK_MIN_SPEECH_MS = 800  # å¤ªçŸ­çš„æ®µä¸ç®—å£°çº¹ï¼ˆé¿å…æŠ–åŠ¨ï¼‰

OUTPUT_JSON = "output/stream_output.json"

# =========================
# 1) æ¨¡å‹åŠ è½½
# =========================
# ASRï¼šæ²¿ç”¨ä½ çš„å®æ—¶æ–¹æ¡ˆ
model_asr = AutoModel(
	model="iic/SenseVoiceSmall",
	trust_remote_code=True,
	device="cuda:0",
	disable_update=True
)

# VADï¼šæ²¿ç”¨ä½ çš„å®æ—¶æ–¹æ¡ˆ
model_vad = AutoModel(
	model="fsmn-vad",
	model_revision="v2.0.4",
	disable_pbar=True,
	max_end_silence_time=MAX_END_SILENCE_MS,
	disable_update=True
)

# SPKï¼šCam++ / CAMPPlusï¼ˆç¦»çº¿æ–¹æ¡ˆé‡Œçš„ spk_modelï¼‰
# æ³¨æ„ï¼šä¸åŒç‰ˆæœ¬ funasr çš„è¾“å‡º key å¯èƒ½ä¸ä¸€è‡´ï¼Œæ‰€ä»¥ä¸‹é¢æœ‰ robust è§£æ
model_spk = AutoModel(
	model="iic/speech_campplus_sv_zh-cn_16k-common",
	device="cuda:0",
	disable_update=True
)


# =========================
# 2) åœ¨çº¿è¯´è¯äººåˆ†é…å™¨
# =========================
def _l2norm(x: np.ndarray, eps: float = 1e-12) -> np.ndarray:
	n = np.linalg.norm(x) + eps
	return x / n


def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:
	a = _l2norm(a)
	b = _l2norm(b)
	return float(np.dot(a, b))


@dataclass
class SpeakerAssigner:
	cos_thresh: float = SPK_COS_THRESH
	centroids: List[np.ndarray] = field(default_factory=list)
	counts: List[int] = field(default_factory=list)
	
	def assign_spk(self, emb: np.ndarray) -> int:
		"""ç»™ä¸€ä¸ª embeddingï¼Œè¿”å› spk_idï¼ˆ0,1,2,...ï¼‰ï¼Œå¹¶åœ¨çº¿æ›´æ–° centroidã€‚"""
		if emb is None:
			return -1
		emb = np.asarray(emb, dtype=np.float32).reshape(-1)
		
		if len(self.centroids) == 0:
			self.centroids.append(_l2norm(emb))
			self.counts.append(1)
			return 0
		
		sims = [cosine_sim(emb, c) for c in self.centroids]
		best_i = int(np.argmax(sims))
		best_s = sims[best_i]
		
		if best_s >= self.cos_thresh:
			# åœ¨çº¿æ›´æ–° centroidï¼šåŠ æƒå¹³å‡å†å½’ä¸€åŒ–
			k = self.counts[best_i]
			new_c = (self.centroids[best_i] * k + _l2norm(emb)) / (k + 1.0)
			self.centroids[best_i] = _l2norm(new_c)
			self.counts[best_i] = k + 1
			return best_i
		
		# æ–° speaker
		self.centroids.append(_l2norm(emb))
		self.counts.append(1)
		return len(self.centroids) - 1


def _to_numpy(x):
	"""æŠŠå„ç§ç±»å‹çš„ embedding è½¬æˆ 1D numpy.float32"""
	if x is None:
		return None
	# torch tensor
	if torch.is_tensor(x):
		return x.detach().float().cpu().numpy().reshape(-1).astype(np.float32)
	# numpy / list
	arr = np.asarray(x, dtype=np.float32)
	return arr.reshape(-1)


def extract_spk_embedding(model, speech_f32: np.ndarray):
	"""
	ä» Cam++ / CAMPPlus è¾“å‡ºä¸­æå– embeddingï¼Œå¹¶å®‰å…¨è½¬ numpyï¼ˆæ”¯æŒ cuda tensorï¼‰
	"""
	try:
		out = model.generate(input=speech_f32)
		if not out:
			return None
		
		obj = out[0] if isinstance(out, list) else out
		
		candidate_keys = [
			"spk_embedding", "speaker_embedding", "embedding", "emb",
			"spk_emb", "xvector", "vector"
		]
		
		# 1) ç›´æ¥å°±æ˜¯ tensor / list / np
		if torch.is_tensor(obj) or isinstance(obj, (list, tuple, np.ndarray)):
			return _to_numpy(obj)
		
		# 2) dictï¼šå¸¸è§ç»“æ„
		if isinstance(obj, dict):
			for k in candidate_keys:
				if k in obj and obj[k] is not None:
					return _to_numpy(obj[k])
			
			# åµŒå¥— dictï¼švalue / outputs / result / data
			for nest_k in ["value", "outputs", "result", "data"]:
				v = obj.get(nest_k)
				if isinstance(v, dict):
					for k in candidate_keys:
						if k in v and v[k] is not None:
							return _to_numpy(v[k])
				# æœ‰çš„ nest ç›´æ¥å°±æ˜¯ tensor
				if v is not None and (torch.is_tensor(v) or isinstance(v, (list, tuple, np.ndarray))):
					# ä½†åªæœ‰å½“å®ƒåƒ embeddingï¼ˆ1D/2Då°å‘é‡ï¼‰æ—¶æ‰æ”¶
					arr = _to_numpy(v)
					if arr is not None and arr.size <= 4096:
						return arr
		
		# 3) å®åœ¨ä¸è¡Œï¼šéå† dict é‡Œçš„æ‰€æœ‰å€¼ï¼Œæ‰¾åˆ°ç¬¬ä¸€ä¸ªâ€œåƒ embeddingâ€çš„å‘é‡
		if isinstance(obj, dict):
			for v in obj.values():
				if v is None:
					continue
				if torch.is_tensor(v) or isinstance(v, (list, tuple, np.ndarray)):
					arr = _to_numpy(v)
					if arr is not None and arr.ndim == 1 and 8 <= arr.size <= 4096:
						return arr
		
		return None
	
	except Exception as e:
		print(f"[WARN] extract_spk_embedding failed: {e}")
		return None


# =========================
# 3) åˆå¹¶è¾“å‡ºå•å…ƒï¼ˆæ¥è‡ªç¦»çº¿é€»è¾‘ï¼‰
# =========================
def merge_units(units: List[Dict[str, Any]], spk: int, text: str, start_ms: int, end_ms: int):
	text = (text or "").strip()
	if not text:
		return
	
	if MERGE_CONTINUOUS_SPK and units:
		last = units[-1]
		same_spk = (last["spk"] == spk)
		gap = start_ms - last["end_ms"]
		if same_spk and gap <= MERGE_GAP_MS:
			last["text"] += text
			last["end_ms"] = end_ms
			return
	
	units.append({
		"spk"     : spk,
		"text"    : text,
		"start_ms": int(start_ms),
		"end_ms"  : int(end_ms),
	})


# =========================
# 4) ä¸»æµç¨‹ï¼šé‡‡é›† -> VAD -> åˆ‡æ®µ -> ASR + SPK -> merge -> JSON
# =========================
def main():
	# PyAudio
	p = pyaudio.PyAudio()
	stream = p.open(
		format=FORMAT,
		channels=CHANNELS,
		rate=RATE,
		input=True,
		frames_per_buffer=CHUNK,
	)
	print("ğŸ™ï¸ Streaming start (Ctrl+C to stop)")
	
	# çŠ¶æ€
	chunk_size = int(CHUNK_MS * RATE / 1000)
	
	audio_buffer = np.array([], dtype=np.float32)  # åŸå§‹é‡‡é›†ç¼“å­˜
	audio_vad = np.array([], dtype=np.float32)  # VAD å‚è€ƒæ—¶é—´è½´ç¼“å­˜
	
	cache_vad: Dict[str, Any] = {}
	cache_asr: Dict[str, Any] = {}
	
	last_vad_beg = -1
	last_vad_end = -1
	offset = 0  # ä½ åŸè„šæœ¬é‡Œçš„ offset é€»è¾‘ï¼šæŠŠ VAD è¾“å‡ºæ˜ å°„åˆ° audio_vad å­æ•°ç»„
	
	# ç»“æœ
	units: List[Dict[str, Any]] = []
	spk_assigner = SpeakerAssigner()
	
	# è®°å½•â€œä»å¼€å§‹åˆ°ç°åœ¨â€çš„æ¯«ç§’
	t0 = time.time()
	
	try:
		while True:
			data = stream.read(CHUNK, exception_on_overflow=False)
			audio_i16 = np.frombuffer(data, dtype=np.int16)
			audio_f32 = audio_i16.astype(np.float32) / 32767.0
			audio_buffer = np.append(audio_buffer, audio_f32)
			
			while len(audio_buffer) >= chunk_size:
				chunk = audio_buffer[:chunk_size]
				audio_buffer = audio_buffer[chunk_size:]
				audio_vad = np.append(audio_vad, chunk)
				
				# ===== VAD streaming =====
				res_vad = model_vad.generate(
					input=chunk,
					cache=cache_vad,
					is_final=False,
					chunk_size=CHUNK_MS
				)
				
				# res_vad[0]["value"] å¯èƒ½æ˜¯ [] / [[beg,-1]] / [[-1,end]] / [[beg,end],...]
				if res_vad and len(res_vad[0].get("value", [])) > 0:
					for seg in res_vad[0]["value"]:
						if seg[0] > -1:
							last_vad_beg = seg[0]
						if seg[1] > -1:
							last_vad_end = seg[1]
						
						# åªæœ‰ beg/end éƒ½æ‹¿åˆ°ï¼Œæ‰åˆ‡æ®µ
						if last_vad_beg > -1 and last_vad_end > -1:
							# æ˜ å°„åˆ° audio_vad çš„å±€éƒ¨åæ ‡ï¼ˆæ²¿ç”¨ä½ åŸå®æ—¶è„šæœ¬ï¼‰
							last_vad_beg -= offset
							last_vad_end -= offset
							offset += last_vad_end
							
							beg = int(last_vad_beg * RATE / 1000)
							end = int(last_vad_end * RATE / 1000)
							speech = audio_vad[beg:end]
							
							seg_dur_ms = int((end - beg) * 1000 / RATE)
							# ===== ASR =====
							asr_out = model_asr.generate(
								input=speech,
								cache=cache_asr,
								language="auto",
								use_itn=True,
							)
							text = ""
							if asr_out and asr_out[0].get("text"):
								text = asr_out[0]["text"]
							
							# ===== SPK =====
							spk_id = -1
							if seg_dur_ms >= SPK_MIN_SPEECH_MS:
								emb = extract_spk_embedding(model_spk, speech)
								if emb is not None:
									spk_id = spk_assigner.assign_spk(emb)
							
							# ä¼°è®¡è¯¥æ®µåœ¨å…¨å±€æ—¶é—´è½´çš„èµ·æ­¢ï¼ˆmsï¼‰
							# è¿™é‡Œç”¨â€œä»å¼€å§‹åˆ°ç°åœ¨â€çš„è¿‘ä¼¼ï¼šoffset å…¶å®å°±æ˜¯ endï¼ˆmsï¼‰åœ¨ audio_vad çš„ç´¯è®¡ï¼Œ
							# æ‰€ä»¥ç”¨ (offset - last_vad_end .. offset) ä½œä¸ºè¯¥æ®µçš„å…¨å±€æ—¶é—´ï¼ˆmsï¼‰
							seg_end_ms_global = offset
							seg_start_ms_global = offset - int(last_vad_end)
							
							# merge + æ‰“å°
							if text.strip():
								merge_units(units, spk_id, text, seg_start_ms_global, seg_end_ms_global)
								print(
									f"[{seg_start_ms_global / 1000:07.3f}-{seg_end_ms_global / 1000:07.3f}] [SPK{spk_id}] {text}")
							
							# æ¸…ç†å·²æ¶ˆè´¹éŸ³é¢‘
							audio_vad = audio_vad[end:]
							last_vad_beg = -1
							last_vad_end = -1
	
	except KeyboardInterrupt:
		print("\nâ¹ï¸ stopped")
	
	finally:
		stream.stop_stream()
		stream.close()
		p.terminate()
		
		with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
			json.dump({"segments": units}, f, ensure_ascii=False, indent=2)
		
		print(f"âœ… saved: {OUTPUT_JSON} (segments={len(units)})")


if __name__ == "__main__":
	main()
