**会议纪要**

**会议主题：** 关于RAG（检索增强生成）技术流程的讨论

**与会人员：** 发言人0， 发言人1

**纪要整理：** AI助手

**日期：** [请补充会议日期]

---

**会议核心内容：**

本次会议主要围绕构建RAG（检索增强生成）应用的技术流程展开讨论，重点解释了从文档处理到最终生成回答的各个环节。

**详细讨论要点：**

1.  **文档预处理与切片：**
    *   **问题：** 处理长文档时，由于模型（如Embedding模型）有输入长度限制，无法一次性处理整个文档。
    *   **解决方案：** 需要对文档进行“切片”，将其切分成多个较小的文本片段（窗口）。
    *   **关键挑战：** 如何切片才能保证语义的完整性？目标是进行“语义聚合度最高”的切割，避免在语义关键处切断，以确保每个切片具有独立且完整的含义。

2.  **文本向量化（Embedding）：**
    *   **过程：** 使用Embedding模型（例如BGE等）将上一步得到的每一个文本切片，转换成一个高维度的向量（数组）。
    *   **原理解释：**
        *   向量在计算机中表现为一个高维数组，数组中的每个数值代表一种特征或权重。
        *   模型在转换时，会考虑文本中每个字/词，并逐步调整向量的权重，最终为整个文本片段生成一个固定的向量表示。这个向量蕴含了该片段的语义信息。
        *   不同的文本片段会生成不同的向量。

3.  **向量存储：**
    *   将生成的所有向量存储到专门的**向量数据库**中（会议中提到了Chroma DB、Milvus等）。
    *   向量数据库类似于传统数据库，但专门用于高效存储和检索向量数据。

4.  **用户查询与检索：**
    *   当用户在应用中提问时：
        *   **步骤一：** 使用同样的Embedding模型将用户的问题也转换成一个向量。
        *   **步骤二：** 在向量数据库中，计算用户问题向量与库中所有存储向量的**余弦相似度**。
        *   **步骤三：** 根据相似度分数，召回最相关的若干个文本片段（通常通过设置`top_k`参数控制数量，例如召回相似度最高的3个）。

5.  **答案生成：**
    *   **步骤一：** 将用户的原问题与检索召回的多个文本片段（已还原为原始文本）组合在一起。
    *   **步骤二：** 将组合后的内容提交给大语言模型（LLM），并配以适当的系统提示（例如：“请根据以下提供的背景信息回答问题”）。
    *   **步骤三：** LLM基于提供的上下文和用户问题，生成最终答案，并返回给用户。

**总结与关键点：**
整个RAG流程可以概括为：**文档切片 -> 向量化 -> 入库 -> 问題向量化 -> 向量检索 -> 上下文组合 -> LLM生成答案**。
会议最后强调，整个流程的起点——**文档切片的质量至关重要**。只有切片切得好，生成的向量才能准确代表语义，后续的检索和回答质量才有保障。
